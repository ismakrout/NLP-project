{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ismailakrout/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ismailakrout/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import math\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from src.utils import *\n",
    "from src.data_preprocessing import *\n",
    "from src.data_processing import *\n",
    "from src.feature_selection import *\n",
    "from src.modelisation_arcticle_1 import *\n",
    "\n",
    "os.chdir('/Users/ismailakrout/Desktop/python/NLP_statapp')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour ne pas charger le NB avec des warnings \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lit la base des speechs\n",
    "# on peut la rendre plus petite pour faire des tests \n",
    "df = read_HouseOfCommons(keep_date=False, rd_lines=True, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ne garde que Labor et Conservative\n",
    "df = keep_parties(df, ['Lab', 'Con'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cleen les titres des speechs\n",
    "df['agenda'] = df['agenda'].apply(clean, args=('unigram',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit las liste cleen des topics\n",
    "list_stem_topics = process_list_BigTech_words(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On filtre le df et on garde uniquement les speechs sur les BigTech\n",
    "df = keep_Bigtech_speeches(df, list_stem_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean, args=('bigram',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait le groupby() par speaker et party par la suite pour ne pas dépasser les 1 000 000 carc \n",
    "df = df.groupby(by=['Speaker', 'party']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UF4ECBXLMsw"
   },
   "source": [
    "On va repérer les 100 mots (puis 500 quand on aura les speeches en entier) qui sont le plus utilisés par chaque parti, et noter leur fréquence d'apparition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freqs_Con = count_freqs(df, 'Con')\n",
    "df_freqs_Lab = count_freqs(df, 'Lab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHkh7aJFRgBl"
   },
   "source": [
    "On a tous les mots utilisés par le parti Conservateur sur les sujets définis, ainsi que leurs fréquences d'apparition. On va retenir les fréquences avec lesquelles chaques mots sont utilisés, par chaque parti, afin de déterminer le chi2 de chaque mot. On se restreint pour l'instant aux partis Con et Lab, fortement majoritaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fTbiLRczPjiF"
   },
   "outputs": [],
   "source": [
    "df_freqs = merge_freq(df_freqs_Con, df_freqs_Lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On effectue le etst du Qi 2 \n",
    "df_freqs = test_qi_2(df_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HafOcHmJHzNm"
   },
   "source": [
    "On a donc ici chaque mot utilisé par les parti sur le sujet délimité, avec le chi2 correspondant. On va se limiter aux 500 premiers mots :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freqs = keep_significatif_word(df_freqs, n=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = selected_words(df_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = construct_df_reg(df, df_freqs, list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize(df, list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = party_str_to_dummy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions linéaires (Modélisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajout des lignes slopes_coeff et intercept\n",
    "\n",
    "df = regress_df(df, list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('01. output/df_500_with_slope_coef_bis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0704e9e7027e30b245531b4e60cb4aaf65b18a7e325eaeb71f06804e85db260f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
