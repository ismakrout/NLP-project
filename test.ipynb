{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrequeant/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexandrequeant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import math\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "path = '/Users/alexandrequeant/Desktop/Statapp/Corp_HouseOfCommons_V2_2010.csv'\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexandrequeant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# **Fonctions de traitement**\n",
    "def read_input(path_1, encod, **kwargs):\n",
    "    dtype_values = kwargs.get('dtype_values', None)\n",
    "    df = pd.read_csv(path, sep=';', encoding=encod, dtype=dtype_values)\n",
    "    return df\n",
    "dtypes = {\n",
    "    'party.facts.id' : str,\n",
    "    'date': object,\n",
    "    'agenda': object,\n",
    "    'speechnumber': int,\n",
    "    'speaker': object,\n",
    "    'party': object,\n",
    "    'party.facts.id': object,\n",
    "    'chair': bool,\n",
    "    'terms': int,\n",
    "    'text': object,\n",
    "}\n",
    "\n",
    "def read_HouseOfCommons(keep_date):\n",
    "    '''\n",
    "    Objectif:\n",
    "    Cette fonction permet de lire la base des parlementaires \n",
    "    elle renvoie le dataFrame preprocessé\n",
    "    Arguments:\n",
    "    keep_date -> bool qui détermine si on supprime la colonne keep_date\n",
    "    '''\n",
    "    df = read_input(path, encod='ISO-8859-1', dtype_values=dtypes)\n",
    "    if keep_date:\n",
    "        df.drop(columns=['Unnamed: 0', 'iso3country', 'parliament', 'party.facts.id', 'speechnumber', 'chair', 'terms'], inplace=True)\n",
    "    else:\n",
    "        df.drop(columns=['Unnamed: 0', 'iso3country', 'parliament', 'party.facts.id', 'speechnumber', 'chair', 'terms', 'date'], inplace=True)\n",
    "    df.rename(columns=\n",
    "        {'speaker': 'Speaker'},\n",
    "        inplace=True\n",
    "    )\n",
    "    return df \n",
    "\n",
    "def keep_rd_lines(df, n):\n",
    "    '''\n",
    "    Objectif:\n",
    "    Cette fonction renvoie n random lines du df  \n",
    "    Arguments:\n",
    "    df -> le DataFrame\n",
    "    n -> le nb de lignes qu'on souhaite garder\n",
    "    '''\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df = df.head(n)\n",
    "    return df\n",
    "\n",
    "def keep_parties(df, list_of_parties):\n",
    "    '''\n",
    "    Objectif:\n",
    "    Cette fonction le df en ne gardant que les partis politiques choisis  \n",
    "    Arguments:\n",
    "    df -> DataFrame : le DataFrame avec les speechs\n",
    "    list_of_parties -> list : liste des partis politiques qu'on souhaite garder\n",
    "    '''\n",
    "    return df.loc[df['party'].isin(list_of_parties)]\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "putback = ['prime', 'officials', 'security', 'news', 'working', 'games', 'jobs', 'campaign', 'services',\n",
    "'civil', 'economic', 'information', 'political', 'election', 'court', 'office', 'vote', 'trump', 'control', 'job', 'price',\n",
    "'donald trump', 'chinese', 'problems', 'concerns', 'minister', 'nation', 'policy', 'data', 'indian', 'congress',\n",
    "'president', 'network', 'american', 'accused', 'government', 'money', 'investigation', 'facebook', \n",
    "'success', 'prices', 'twitter', 'book', 'politics',  'justice', 'claims', 'russia', 'law', 'technology',\n",
    "'content', 'union', 'european', 'workers']\n",
    "\n",
    "def construct_list_stopwords(list_putback_words=putback):\n",
    "  '''\n",
    "  Objectif:\n",
    "  On importe la liste des stopwords :\n",
    "  On a donc un dataframe des stopwords. On en tire une liste simple des stopwords, où l'on remet cependant des stopwords jugés significatifs, comme expliqué pour le traitement des journaux :\n",
    "  \n",
    "  Arguments:\n",
    "  list_putback_words -> list des mot jugés finalement signifcatifs \n",
    "  '''\n",
    "  df_stopwords = pd.read_csv('/Users/alexandrequeant/Desktop/Statapp/sw1k.csv',\n",
    "                names=['word', 'frequency', 'presence', 'doc_size_sum', 'type'],\n",
    "                encoding='latin-1').drop(index=0)\n",
    "  stopwords = df_stopwords['word'].unique()\n",
    "  stopwords = list(set(stopwords) - set(list_putback_words))\n",
    "  return stopwords\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.disable_pipes([\"tagger\", \"parser\"])\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def extract_bigrams(n_grams):\n",
    "    bigrams = []\n",
    "    for i in range(len(n_grams)-1):\n",
    "        bigram = f\"{n_grams[i]} {n_grams[i+1]}\"\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "def clean(text, list_stopwords, gram):\n",
    "  '''\n",
    "  Objectif:\n",
    "  On va cleen le text\n",
    "  On utilise la librairie SpaCy comme pour le traitement des journaux, et on enlève les nombres et la ponctuation avec la méthode translate. \n",
    "  \n",
    "  Arguments:\n",
    "  text -> str: le texte sur lequel on fait le processing\n",
    "  list_stopwords -> list: mots à écarter !\n",
    "  '''\n",
    "  text = str(text).lower()\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  text = text.translate(str.maketrans('', '', string.digits)) \n",
    "  # Tokenisation\n",
    "  tokens = word_tokenize(text)\n",
    "  # Enlever les caractères qui ne sont pas des lettres\n",
    "  tokens = [re.sub('[^a-zA-Z]', '', token) for token in tokens]\n",
    "  # Stemming\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "  filtered_words = [word for word in tokens_stemmed if not word.lower() in list(english_stopwords )+ list(stop_words)]\n",
    "  # Transformation en bigramme\n",
    "  if gram == 'bigram':\n",
    "    filtered_words = extract_bigrams(filtered_words)\n",
    "  return filtered_words\n",
    "\n",
    "technology=['technology','innovat','computer','high tech|high-tech','science','engineering']\n",
    "consumer_protection=['privacy','data leak','leak','fake news',' safety','decept','defective','hack']\n",
    "firms=['google','alphabet','apple','facebook','meta','amazon','microsoft']\n",
    "products=['chrome','incognito','youtube','nexus','pixel','google drive','gmail','glass','street view','buzz','fitbit',\n",
    " 'maps', 'doodle','play','translate','search', 'google news','nest hub','xl','nest','chromecast','stadia','hub',\n",
    " 'marshmallow','lollipop','cloud','waymo','earth','engine',\n",
    "\n",
    "'apple pay','apple watch','iphone','ipad','ipod','iwatch','macbook','macbook pro', 'macbook air','mac',\n",
    " 'imac','airpods','ios','siri','icloud','apple tv','apple music','app store', 'safari','x','app','apple store',\n",
    " 'xr', 'xs', 'se','iphones','itunes','ibook','plus','pro','max','mini','os','airtag','airtags','arcade','homepod',\n",
    "'keynote','ipados','id','foxconn','facetime','beat','stalk',\n",
    "\n",
    "'messenger','instagram', 'whatsapp','page','feed','oculus',\n",
    "\n",
    "'prime', 'kindle',\n",
    "'publishing','amazon prime','amazon drive','amazon video','amazon business','amazon web service',\n",
    "'amazon cloud', 'alexa','echo dot','echo','dot', 'delivery', 'amazon uk','unlimited', 'episode','foods','grocery', \n",
    "'grand tour','grand','tour','viking', 'vikings','argo','argos','macmillan', 'dvd','clarkson','lord','ring','hair','skin','vacuum',\n",
    "'pre','beer','drake','spark','kart','dog','twitch','cat','xo','matthew','stafford','ratchet','clank',\n",
    "'swagway','album','mouse','showbiz','beauty','guardian','batman','arkham','gc','hair','skin','shirt',\n",
    "'lovefilm','mirzapur','cast','audio','drama','movie','jack ryan','actor','character','lucifer','outlander',\n",
    "'premier','super mario','sky','channel','voyage',\n",
    "\n",
    "\n",
    "'windows','window','xp','surface','xbox','studio','microsoft office', 'office','word','cortana', 'surface pro','teams',  'playstation',\n",
    "'microsoft edge', 'edge', 'gear','outlook','halo','skype','kinect','internet explorer','explorer','ie','bing','xcloud','hololens',\n",
    "'forza','ori','scarlett','scorpio','wordperfect','valhalla','onedrive','games gold','lumia','azure',\n",
    "'assassin creed','assassin','creed','minecraft','yammer','warcraft','tay']\n",
    "\n",
    "\n",
    "\n",
    "ceos=['sundar','pichai','eric','schmidt','steve jobs','tim cook','mark zuckerberg','andy jassy','jeff','bezos','satya', 'nadella','bill gates',\n",
    "      'gates','steve job','steve','tim', 'cook','zuckerberg','ceo','tim cook ','steve ballmer','ballmer','elop',\n",
    "      'schiller','fadell','phil spencer','spencer','mcspirit','sandberg','paul','allen','larry hryb','hryb']\n",
    "      \n",
    "\n",
    "types=['tablet','mobile', 'laptop', 'pc', 'computer', 'desktop','smartphone', 'smartwatch', 'search engine', 'software','hardware',\n",
    "               'machine', 'browser','ebook', 'book',  'reader',  'console', 'headphone', 'earbud','bud','store','music',\n",
    "              'gaming', 'operating','streaming','title','chatbot']\n",
    "\n",
    "\n",
    "\n",
    "competitor=['samsung', 'galaxy',  'twitter','tiktok', 'switch','sony', 'asos', 'activision blizzard', 'activision','blizzard',\n",
    "            'nintendo','snes', 'netflix','android','yahoo','nokia','huawei','motorola','htc','blackberry','oppo','oneplus','rim','symbian','bbc','morrison','spotify'] \n",
    "\n",
    "\n",
    "configue=['device','feature','battery','screen','sound','gb','g','k','mm','chip','processor','design','display','touch','ram',\n",
    "          'inch','keyboard','camera','handset','speaker','button','touchscreen','storage', 'data']\n",
    "\n",
    "\n",
    "celebrity=['dubost','neymar','amanda','beyonce','blur','richard','hammond','ranj','jeremy clarkson', 'jeremy','momoa',\n",
    "           'jared','aniston','smith','kim','tony','tom','sophie','oasis','trio','sharon','betty','raoul','moat','lauren','andrew',\n",
    "           'samuel gibbs','samuel','gibbs','van','gaal']\n",
    "\n",
    "topics = celebrity + configue + competitor + types + ceos + products + firms + consumer_protection + technology\n",
    "\n",
    "def process_list_BigTech_words(topics):\n",
    "  '''\n",
    "  Objectifs:\n",
    "  Obtient une liste cleen des topics BigTech traités\n",
    "  Arguments:\n",
    "  topics -> list : liste des topics de la bigTech \n",
    "  '''\n",
    "  string_of_topics = ' '.join(topics)\n",
    "  stemmer = SnowballStemmer(language='english')\n",
    "  string_of_topics = stemmer.stem(string_of_topics)\n",
    "  list_stem_topics = string_of_topics.split(' ')\n",
    "  return list_stem_topics\n",
    "\n",
    "def lines_to_keep(titre, liste_big_tech):\n",
    "  '''\n",
    "  Objectifs:\n",
    "  prend en input le titre du speech\n",
    "  retourne un booleen qui indique si le speech est en lien avec le domaine de la big tech\n",
    "  Arguments:\n",
    "  titre -> list : titre du speech étudié\n",
    "  liste_big_tech -> set : set des mots en lien avec la BigTech\n",
    "  '''\n",
    "  if len(set(titre) & liste_big_tech) > 0:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "def keep_Bigtech_speeches(df, list_stem_topics):\n",
    "  '''\n",
    "  Objectifs:\n",
    "  prend en input le DataFrame des inputs \n",
    "  renvoie le df contenant uniquement les lignes contenant des speechs en lien avec la BigTech\n",
    "  Arguments:\n",
    "  df -> DataFrame : Dataframe des inputs\n",
    "  list_stem_topics -> list : liste des mots en lien avec la BigTech\n",
    "  '''\n",
    "  set_stem_topics = set(list_stem_topics)\n",
    "  df['lines_to_keep'] = df['agenda'].apply(lines_to_keep, args=(set_stem_topics,))\n",
    "  df = df.loc[df['lines_to_keep']]\n",
    "  df.drop(columns=['agenda', 'lines_to_keep'], inplace=True)\n",
    "  return df \n",
    "\n",
    "def count_freqs(df, party):\n",
    "  '''\n",
    "  Objectifs:\n",
    "  prend en input le DataFrame\n",
    "  renvoie le df des fréquences pour un parti donné\n",
    "  Arguments:\n",
    "  df -> DataFrame : Dataframe \n",
    "  party -> str : le parti politique pour lequel on souhaite les freqs \n",
    "  '''  \n",
    "  aux = df[['party', 'text']] \n",
    "  list_of_words = pd.Series(aux.groupby(by=['party']).sum().loc[party, 'text'])\n",
    "  freq_df = pd.DataFrame(list_of_words.value_counts()) #, columns=[f'freq_{party}'])\n",
    "  freq_df = freq_df.reset_index().rename(columns={'index':\"words\"})\n",
    "  return freq_df\n",
    "\n",
    "def merge_freq(df_1, df_2):\n",
    "  '''\n",
    "  Objectifs:\n",
    "  permet de faire le merge entre les freq_df des différents partis\n",
    "  Arguments:\n",
    "  df_1 -> DataFrame : Dataframe \n",
    "  df_2 -> DataFrame : Dataframe \n",
    "  '''  \n",
    "  df_freqs = pd.merge(\n",
    "    df_1,\n",
    "    df_2,\n",
    "    how='outer',\n",
    "    on=['words'],\n",
    "  )\n",
    "  return df_freqs\n",
    "\n",
    "\n",
    "def count_liste(list, mot):\n",
    "    '''\n",
    "    Objectifs:\n",
    "    calcule la fréquence de mot dans la list\n",
    "    Arguments:\n",
    "    list -> list : la liste qu'on étudie \n",
    "    mot : le mot dont on veut calculer la fréquence\n",
    "    '''\n",
    "    if type(list) == int : \n",
    "      return 0\n",
    "    return list.count(mot)\n",
    "\n",
    "def selected_words(df_freqs):\n",
    "    return df_freqs['words'].unique()\n",
    "\n",
    "def construct_df_reg(df, df_freqs, list_of_words):\n",
    "    '''\n",
    "    Objectifs:\n",
    "    Construire le df qui va nous aider à faire nos régressions \n",
    "    Arguments:\n",
    "    df_freqs -> DataFrame : le df des freqs\n",
    "    '''\n",
    "    for word in list_of_words:\n",
    "        df[f'{word}'] = df['text'].apply(count_liste, args=(word,))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit la liste des mots jugés non significatifs\n",
    "stop_words = construct_list_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Guardian = pd.read_csv('/Users/alexandrequeant/Desktop/Statapp/newSplit_topics_bigfive_theGuardian_with_sentibert_post2010.csv')\n",
    "df_DE = pd.read_csv('/Users/alexandrequeant/Desktop/Statapp/new_bigfive_DailyExpress_with_sentibert_post2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_1(df, party):\n",
    "  df = df[['author', 'fulltext']]\n",
    "  if party == 'Guardian' :\n",
    "    df['party'] = 'Lab'\n",
    "  if party == 'DE' :\n",
    "    df['party'] = 'Con'\n",
    "  df.rename(columns=\n",
    "          {'author' : 'Speaker', 'fulltext':'text'}, inplace=True)\n",
    "  return df\n",
    "\n",
    "def treat_2(df_1,df_2):\n",
    "  df = pd.concat([df_1, df_2])\n",
    "  df.dropna().reset_index(drop=True)\n",
    "  return df\n",
    "\n",
    "def treat_3(df):\n",
    "  df['text'] = df['text'].apply(clean, args=(stop_words,'bigram'))\n",
    "  return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/djr3ybxn3_d3j76smv6vqyfh0000gn/T/ipykernel_34806/640606626.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['party'] = 'Lab'\n",
      "/var/folders/6x/djr3ybxn3_d3j76smv6vqyfh0000gn/T/ipykernel_34806/640606626.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns=\n",
      "/var/folders/6x/djr3ybxn3_d3j76smv6vqyfh0000gn/T/ipykernel_34806/640606626.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['party'] = 'Con'\n",
      "/var/folders/6x/djr3ybxn3_d3j76smv6vqyfh0000gn/T/ipykernel_34806/640606626.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns=\n"
     ]
    }
   ],
   "source": [
    "df_Guardian = treat_1(df_Guardian, 'Guardian')\n",
    "df_DE = treat_1(df_DE, 'DE')\n",
    "df_newspaper = treat_2(df_Guardian, df_DE)\n",
    "df_newspaper = df_newspaper.sample(frac=1).reset_index(drop=True)\n",
    "df_newspaper = df_newspaper.head(100)\n",
    "df_newspaper = treat_3(df_newspaper)\n",
    "df_newspaper_treated = df_newspaper\n",
    "df_newspaper_treated = df_newspaper_treated.groupby(by=['Speaker', 'party']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_newspaper_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By Aaron Brown</td>\n",
       "      <td>Con</td>\n",
       "      <td>[googl initi, initi remind, remind favourit, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By Alex Davies</td>\n",
       "      <td>Con</td>\n",
       "      <td>[fan grand, grand tour, tour wait, wait episod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By Amani Hughes</td>\n",
       "      <td>Con</td>\n",
       "      <td>[chemist celebr, celebr hi, hi th, th birthday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By Carly Read</td>\n",
       "      <td>Con</td>\n",
       "      <td>[wa facebook, facebook appear, appear cross, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By Charlie Moloney</td>\n",
       "      <td>Con</td>\n",
       "      <td>[despit coverag, coverag brexit, brexit publis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>By Colin Bateman in Perth</td>\n",
       "      <td>Con</td>\n",
       "      <td>[con reveal, reveal tech, tech guru, guru stev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>By David Dawkins</td>\n",
       "      <td>Con</td>\n",
       "      <td>[facebook actual, actual finish, finish green,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>By David Snelling</td>\n",
       "      <td>Con</td>\n",
       "      <td>[appl ,  iphon, iphon x, x arriv, arriv store,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>By Dion Dassanayake</td>\n",
       "      <td>Con</td>\n",
       "      <td>[updat googl, googl spokesperson, spokesperson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>By Emma Nolan</td>\n",
       "      <td>Con</td>\n",
       "      <td>[ha arriv, arriv droughtland, droughtland much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>By Felicity Thistlethwaite</td>\n",
       "      <td>Con</td>\n",
       "      <td>[barack hi, hi blog, blog financi, financi bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>By Gary Jones</td>\n",
       "      <td>Con</td>\n",
       "      <td>[industri announc, announc halo, halo onlin, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>By George Simpson</td>\n",
       "      <td>Con</td>\n",
       "      <td>[sinc launch, launch disney, disney marvel, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>By Izzy Horsefield</td>\n",
       "      <td>Con</td>\n",
       "      <td>[blue repeatedli, repeatedli link, link defend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>By Jessica O'Donnell</td>\n",
       "      <td>Con</td>\n",
       "      <td>[review thi, thi electr, electr toothbrush, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>By Joseph Carey</td>\n",
       "      <td>Con</td>\n",
       "      <td>[android googl, googl mobil, mobil oper, oper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>By Kirsty McCormack</td>\n",
       "      <td>Con</td>\n",
       "      <td>[websit ha, ha introduc, introduc fee, fee use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>By Lauren O'Callaghan</td>\n",
       "      <td>Con</td>\n",
       "      <td>[googl map, map window, window wellknown, well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>By Liam Doyle</td>\n",
       "      <td>Con</td>\n",
       "      <td>[facebook investor, investor shock, shock wa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>By Liam Martin</td>\n",
       "      <td>Con</td>\n",
       "      <td>[nintendo classic, classic mini, mini ne, ne w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>By Lloyd Coombes</td>\n",
       "      <td>Con</td>\n",
       "      <td>[iphon christma, christma upon, upon cant, can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>By Lorna Codrai</td>\n",
       "      <td>Con</td>\n",
       "      <td>[famou trio, trio drop, drop episod, episod gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>By Luke Edwards</td>\n",
       "      <td>Con</td>\n",
       "      <td>[googl chrome, chrome browser, browser ha, ha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>By Mikael McKenzie</td>\n",
       "      <td>Con</td>\n",
       "      <td>[djokov ha, ha befor, befor delboni, delboni k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>By Neela Debnath</td>\n",
       "      <td>Con</td>\n",
       "      <td>[vike avail, avail netflix, netflix uk, uk uk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>By Oliver Pritchard-Jones</td>\n",
       "      <td>Con</td>\n",
       "      <td>[mcvay execut, execut pact, pact bodi, bodi br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>By Peter Henn</td>\n",
       "      <td>Con</td>\n",
       "      <td>[motheroftwo kymberley, kymberley myle, myle g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>By Sam Stevenson</td>\n",
       "      <td>Con</td>\n",
       "      <td>[bbc uphil, uphil battl, battl stream, stream ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>By Ted Jeffery</td>\n",
       "      <td>Con</td>\n",
       "      <td>[thi financi, financi seattl, seattl tech, tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>By Tom Fish</td>\n",
       "      <td>Con</td>\n",
       "      <td>[facebook continu, continu strength, strength ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Adrian Horton</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[amazon ,  panic, panic episod, episod seri, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Alex Hern</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[appl reportedli, reportedli drop, drop intel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Alexandra Topping</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[politician advertis, advertis warn, warn goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Alison Flood</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[bestsel author, author includ, includ lee, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Amanda Meade</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[journalist poll, poll cover, cover pandem, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Calum Marsh</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[sometim earli, earli dalla, dalla texa, texa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Charles Arthur</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[thi googl, googl doodl, doodl googl, googl cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Dan Tynan</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[facebook network, network ,  becam, becam del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Emma Graham-Harrison</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[facebook ,  execut, execut zuckerberg, zucker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Felix Salmon</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[scifi geniu, geniu inventor, inventor type, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Hannah Marriott</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[mission quicken, quicken appl, appl ,  econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Ian Dunt</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[wa onli, onli thi, thi morn, morn lovefilm, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Josh Halliday</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[quick burst, burst link, link chew, chew pick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Julia Carrie Wong</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[facebook expos, expos internet, internet serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Juliette Garside</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[appl investig, investig italian, italian judi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Katie Puckrik</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[isnt familiar, familiar joy, joy unbox, unbox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Mark Sweney</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[compani jeremi, jeremi clarkson, clarkson hi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Nils Pratley</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[facebook ,  libra, libra digit, digit currenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Rajeev Syal</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[amazon googl, googl starbuck, starbuck accus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Robert McCrum</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[book decad, decad centuri, centuri ha, ha cul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Roy Greenslade</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[appear acclaim, acclaim waterg, waterg invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Samuel Gibbs</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[appl ha, ha push, push updat, updat iphon, ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Sarah Butler</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[amazon transform, transform anoth, anoth sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Stuart Dredge</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[univers ha, ha attract, attract activ, activ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Subhajit Banerjee</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[reader guardian, guardian content, content an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Speaker party   \n",
       "0                By Aaron Brown   Con  \\\n",
       "1               By Alex Davies    Con   \n",
       "2              By Amani Hughes    Con   \n",
       "3                 By Carly Read   Con   \n",
       "4            By Charlie Moloney   Con   \n",
       "5     By Colin Bateman in Perth   Con   \n",
       "6              By David Dawkins   Con   \n",
       "7             By David Snelling   Con   \n",
       "8           By Dion Dassanayake   Con   \n",
       "9                 By Emma Nolan   Con   \n",
       "10   By Felicity Thistlethwaite   Con   \n",
       "11                By Gary Jones   Con   \n",
       "12            By George Simpson   Con   \n",
       "13           By Izzy Horsefield   Con   \n",
       "14         By Jessica O'Donnell   Con   \n",
       "15              By Joseph Carey   Con   \n",
       "16          By Kirsty McCormack   Con   \n",
       "17        By Lauren O'Callaghan   Con   \n",
       "18                By Liam Doyle   Con   \n",
       "19               By Liam Martin   Con   \n",
       "20             By Lloyd Coombes   Con   \n",
       "21              By Lorna Codrai   Con   \n",
       "22              By Luke Edwards   Con   \n",
       "23           By Mikael McKenzie   Con   \n",
       "24             By Neela Debnath   Con   \n",
       "25    By Oliver Pritchard-Jones   Con   \n",
       "26                By Peter Henn   Con   \n",
       "27             By Sam Stevenson   Con   \n",
       "28               By Ted Jeffery   Con   \n",
       "29                  By Tom Fish   Con   \n",
       "30                Adrian Horton   Lab   \n",
       "31                    Alex Hern   Lab   \n",
       "32            Alexandra Topping   Lab   \n",
       "33                 Alison Flood   Lab   \n",
       "34                 Amanda Meade   Lab   \n",
       "35                  Calum Marsh   Lab   \n",
       "36               Charles Arthur   Lab   \n",
       "37                    Dan Tynan   Lab   \n",
       "38         Emma Graham-Harrison   Lab   \n",
       "39                 Felix Salmon   Lab   \n",
       "40              Hannah Marriott   Lab   \n",
       "41                     Ian Dunt   Lab   \n",
       "42                Josh Halliday   Lab   \n",
       "43            Julia Carrie Wong   Lab   \n",
       "44             Juliette Garside   Lab   \n",
       "45                Katie Puckrik   Lab   \n",
       "46                  Mark Sweney   Lab   \n",
       "47                 Nils Pratley   Lab   \n",
       "48                  Rajeev Syal   Lab   \n",
       "49                Robert McCrum   Lab   \n",
       "50               Roy Greenslade   Lab   \n",
       "51                 Samuel Gibbs   Lab   \n",
       "52                 Sarah Butler   Lab   \n",
       "53                Stuart Dredge   Lab   \n",
       "54            Subhajit Banerjee   Lab   \n",
       "\n",
       "                                                 text  \n",
       "0   [googl initi, initi remind, remind favourit, f...  \n",
       "1   [fan grand, grand tour, tour wait, wait episod...  \n",
       "2   [chemist celebr, celebr hi, hi th, th birthday...  \n",
       "3   [wa facebook, facebook appear, appear cross, c...  \n",
       "4   [despit coverag, coverag brexit, brexit publis...  \n",
       "5   [con reveal, reveal tech, tech guru, guru stev...  \n",
       "6   [facebook actual, actual finish, finish green,...  \n",
       "7   [appl ,  iphon, iphon x, x arriv, arriv store,...  \n",
       "8   [updat googl, googl spokesperson, spokesperson...  \n",
       "9   [ha arriv, arriv droughtland, droughtland much...  \n",
       "10  [barack hi, hi blog, blog financi, financi bra...  \n",
       "11  [industri announc, announc halo, halo onlin, o...  \n",
       "12  [sinc launch, launch disney, disney marvel, ma...  \n",
       "13  [blue repeatedli, repeatedli link, link defend...  \n",
       "14  [review thi, thi electr, electr toothbrush, to...  \n",
       "15  [android googl, googl mobil, mobil oper, oper ...  \n",
       "16  [websit ha, ha introduc, introduc fee, fee use...  \n",
       "17  [googl map, map window, window wellknown, well...  \n",
       "18  [facebook investor, investor shock, shock wa, ...  \n",
       "19  [nintendo classic, classic mini, mini ne, ne w...  \n",
       "20  [iphon christma, christma upon, upon cant, can...  \n",
       "21  [famou trio, trio drop, drop episod, episod gr...  \n",
       "22  [googl chrome, chrome browser, browser ha, ha ...  \n",
       "23  [djokov ha, ha befor, befor delboni, delboni k...  \n",
       "24  [vike avail, avail netflix, netflix uk, uk uk,...  \n",
       "25  [mcvay execut, execut pact, pact bodi, bodi br...  \n",
       "26  [motheroftwo kymberley, kymberley myle, myle g...  \n",
       "27  [bbc uphil, uphil battl, battl stream, stream ...  \n",
       "28  [thi financi, financi seattl, seattl tech, tec...  \n",
       "29  [facebook continu, continu strength, strength ...  \n",
       "30  [amazon ,  panic, panic episod, episod seri, s...  \n",
       "31  [appl reportedli, reportedli drop, drop intel,...  \n",
       "32  [politician advertis, advertis warn, warn goog...  \n",
       "33  [bestsel author, author includ, includ lee, le...  \n",
       "34  [journalist poll, poll cover, cover pandem, pa...  \n",
       "35  [sometim earli, earli dalla, dalla texa, texa ...  \n",
       "36  [thi googl, googl doodl, doodl googl, googl cl...  \n",
       "37  [facebook network, network ,  becam, becam del...  \n",
       "38  [facebook ,  execut, execut zuckerberg, zucker...  \n",
       "39  [scifi geniu, geniu inventor, inventor type, t...  \n",
       "40  [mission quicken, quicken appl, appl ,  econom...  \n",
       "41  [wa onli, onli thi, thi morn, morn lovefilm, l...  \n",
       "42  [quick burst, burst link, link chew, chew pick...  \n",
       "43  [facebook expos, expos internet, internet serv...  \n",
       "44  [appl investig, investig italian, italian judi...  \n",
       "45  [isnt familiar, familiar joy, joy unbox, unbox...  \n",
       "46  [compani jeremi, jeremi clarkson, clarkson hi,...  \n",
       "47  [facebook ,  libra, libra digit, digit currenc...  \n",
       "48  [amazon googl, googl starbuck, starbuck accus,...  \n",
       "49  [book decad, decad centuri, centuri ha, ha cul...  \n",
       "50  [appear acclaim, acclaim waterg, waterg invest...  \n",
       "51  [appl ha, ha push, push updat, updat iphon, ip...  \n",
       "52  [amazon transform, transform anoth, anoth sect...  \n",
       "53  [univers ha, ha attract, attract activ, activ ...  \n",
       "54  [reader guardian, guardian content, content an...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_newspaper_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux =  df_newspaper_treated[['party', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compani</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app store</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake news</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legitimaci alibaba</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alibaba aliyun</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aliyun exploit</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exploit natur</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>googl appl</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10897 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count\n",
       "                       36\n",
       "compani                21\n",
       "facebook               16\n",
       "app store              15\n",
       "fake news              13\n",
       "...                   ...\n",
       "legitimaci alibaba      1\n",
       "alibaba aliyun          1\n",
       "aliyun exploit          1\n",
       "exploit natur           1\n",
       "googl appl              1\n",
       "\n",
       "[10897 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party = 'Lab'\n",
    "\n",
    "list_of_words = pd.Series(aux.groupby(by=['party']).sum().loc['Lab', 'text'])\n",
    "freq_df = pd.DataFrame(list_of_words.value_counts(), columns=[f'freq_{party}']) #c'est donc cette ligne qui ne marche pas \n",
    "list_of_words.value_counts() #ça marche\n",
    "pd.DataFrame(list_of_words.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freqs_Con = count_freqs(df_newspaper_treated, 'Con')\n",
    "df_freqs_Lab = count_freqs(df_newspaper_treated, 'Lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_freqs_Con_work' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_freqs \u001b[39m=\u001b[39m merge_freq(df_freqs_Con_work, df_freqs_Lab_work)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_freqs_Con_work' is not defined"
     ]
    }
   ],
   "source": [
    "df_freqs = merge_freq(df_freqs_Con_work, df_freqs_Lab_work)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
